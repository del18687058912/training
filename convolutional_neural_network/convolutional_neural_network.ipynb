{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 深度学习（Deep Learning）培训"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## 根本目的：为了找出解决问题的函数。    问题-> **f**-> 解\n",
    "  \n",
    "### 找寻该函数的一种方法：神经网络，它是机器学习的其中一种方法 \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 目录\n",
    "### 神经网络（Neural Networks）\n",
    "### ➡️ 卷积神经网络（Convolutional Neural Networks）⬅️\n",
    "### 循环神经网络（Recurrent Neural Networks）\n",
    "### 生成对抗神经网络（Generative Adversarial Networks）\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 卷积神经网络（Convolutional Neural Networks）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 模型评价和验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 训练集和测试集\n",
    "训练集：用来训练模型  \n",
    "测试集：用来评价模型的好坏，永远不能用测试集来训练，也要防止测试集变相泄漏到训练集中。（例如：根据测试集的好坏来调参）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#使用scikit-learn切分数据集\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=np.random.random((4,4))\n",
    "Y=np.random.randint(2,size=(4,1))\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52752549,  0.11075408,  0.04488648,  0.94831371],\n",
       "       [ 0.945337  ,  0.57129645,  0.76167556,  0.86129855],\n",
       "       [ 0.62836435,  0.46990505,  0.04987091,  0.28678808]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89174861,  0.70495976,  0.15363172,  0.56980621]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 评价分类\n",
    "##### 混淆矩阵（Confusion Matrix）\n",
    "又称为可能性表格或是错误矩阵。可视化的看看分类效果，用来评价分类（Classification）\n",
    "\n",
    "|     |被诊断有病      |被诊断无病     |\n",
    "|---|--------------|--------------|\n",
    "|有病|True Positive |False Negative|\n",
    "|没病|False Positive|True Negative |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "score=accuracy_score(y_true,y_pred) # 0和3是对的，1和2错了，所以0.5\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 评价回归\n",
    "##### 平均绝对误差（Mean Absolute Error）\n",
    "但是有缺点就是无法微分，不能应用梯度下降的误差函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X=np.array([1,2,3,4]).reshape((-1,1)) # 列向量,-1表示自动推理该位置有多少个数\n",
    "Y=np.array([1,2,3,4]).reshape((-1,1)) # 列向量\n",
    "\n",
    "regression=LinearRegression()\n",
    "regression.fit(X,Y)\n",
    "\n",
    "guesses=regression.predict(X)\n",
    "\n",
    "error=mean_absolute_error(Y,guesses)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 均方差（Mean Squared Error）\n",
    "可微分，适合做可梯度下降的误差函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X=np.array([1,2,3,4]).reshape((-1,1)) # 列向量,-1表示自动推理该位置有多少个数\n",
    "Y=np.array([1,2,3,4]).reshape((-1,1)) # 列向量\n",
    "\n",
    "regression=LinearRegression()\n",
    "regression.fit(X,Y)\n",
    "\n",
    "guesses=regression.predict(X)\n",
    "\n",
    "error=mean_squared_error(Y,guesses)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### $R^2$ 决定系数（$R^2$ Score）\n",
    "又叫拟合优度\n",
    "<img src=\"r2_score.jpg\" width=450 height=450 />\n",
    "最简模型的误差是最大的  \n",
    "好的模型：越接近1，因为模型的误差相对于最简模型越小则该项越接近0\n",
    "\n",
    "公式：  \n",
    "If $\\hat{y}_i$ is the predicted value of the i-th sample and y_i is the corresponding true value, then the score R² estimated over $n_{\\text{samples}}$ is defined as\n",
    "\n",
    "$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n_{\\text{samples}} - 1} (y_i - \\hat{y}_i)^2}{\\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\bar{y})^2}$\n",
    "\n",
    "where $\\bar{y} =  \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}} - 1} y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90785714285714292"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_true=np.array([1,2,4])\n",
    "y_pred=np.array([1.3,2.5,3.7])\n",
    "score=r2_score(y_true,y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 模型复杂度图（Model Complexity Graph）\n",
    "欠拟合（underfitting）：训练集上表现的不好。 Error due to bias  \n",
    "* 模型小，收到数据影响小，variance比较小，但是可能模拟不到真实情况，bias整个都偏离了正确的   \n",
    "\n",
    "过拟合（overfitting）：训练集上变现的太好，以至于试图记住训练集。 Error due to variance\n",
    "* 模型大，更容易存在符合真实情况的权重使模型符合真实情况，bias更准，但是更容易收到数据带来的误差影响，variance更大\n",
    "\n",
    "<img src=\"error_from_where.jpg\" width=450 height=450 />\n",
    "红色是每次取样后的拟合的线，蓝色是全部红线平均后的线，黑色是真实的\n",
    "<img src=\"error_from_which.jpg\" width=450 height=450 />\n",
    "\n",
    "知道了欠拟合和过拟合的情况，就能运用验证集来挑选模型的拟合程度\n",
    "<img src=\"cross_validation.jpg\" width=450 height=450 />\n",
    "<img src=\"model_complexity_graph.jpg\" width=550 height=550 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### K-Fold Cross Validation\n",
    "<img src=\"k_fold_cross_validation.jpg\" width=450 height=450 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN DATA: [[1, 2], [3, 4]] TRAIN LABEL: [3, 4] TEST: [[1, 2], [3, 4]] TEST LABEL: [1, 2]\n",
      "TRAIN: [0 1] TEST: [2 3]\n",
      "TRAIN DATA: [[1, 2], [3, 4]] TRAIN LABEL: [1, 2] TEST: [[1, 2], [3, 4]] TEST LABEL: [3, 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "\n",
    "kf = KFold(n_splits=2,shuffle=False)  #做两次K-Fold\n",
    "for train_index, test_index in kf.split(X):  #每次取出对应的索引\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"TRAIN DATA:\", X_train.tolist(), \"TRAIN LABEL:\",y_train.tolist(), \"TEST:\", X_test.tolist(),\"TEST LABEL:\",y_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 文本情感分析\n",
    "[Sentiment Analysis with Numpy](https://github.com/udacity/deep-learning/tree/master/sentiment-network): Andrew Trask leads you through building a sentiment analysis model, predicting if some text is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Intro to TFLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 新的激活函数（Activation functions）\n",
    "Sigmoid作激活函数  \n",
    "缺点：每经过一层，衰减的厉害\n",
    "<img src=\"derivative_sigmoid.jpg\" width=450 height=450 />\n",
    "\n",
    "普通问题：  \n",
    "Rectified Linear Units简称ReLUs作激活函数，用来替代Sigmoid函数，他的微分为1，不会衰减。  \n",
    "公式：  \n",
    "$f(x)=max(x,0)$\n",
    "<img src=\"relu.jpg\" width=450 height=450 />\n",
    "缺点：\n",
    "要控制好learning rate，否则大的梯度会导致ReLUs的神经元的权重变成0，不再对数据有反应，相当于该神经元死亡\n",
    "\n",
    "面对多分类的问题：  \n",
    "通常使用Softmax作最后一层，输出层的激活函数\n",
    "<img src=\"softmax.jpg\" width=450 height=450 />\n",
    "将普通的输出，转化为输出之和为1的概率\n",
    "公式：  \n",
    "$ \\sigma(z)_j = \\frac {e^{z_j}}{\\sum_{k=1}^K e^{z_k}} $ for $ j = 1,...,K $\n",
    "<img src=\"softmax_math.jpg\" width=450 height=450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 新的误差函数（Loss Function）\n",
    "##### 分类交叉熵（Categorical Cross-Entropy）\n",
    "独热码（one-hot encoding）：用来表示目前是多种状态的哪一个状态  \n",
    "如：  \n",
    "标签 $ y = [0,0,0,0,1,0,0,0,0,0] $   \n",
    "预测值 $ \\hat y = [0.047,0.048,0.061,0.07,0.330,0.062,0.001,0.213,0.013,0.150] $\n",
    "\n",
    "交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。\n",
    "公式及计算方法：\n",
    "<img src=\"cross_entropy_calculation.jpg\" width=450 height=450 />\n",
    "\n",
    "分类交叉熵经常和输出层是Softmax配套使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 简单的情感分析技巧\n",
    "##### Bag of Words\n",
    "\"the fox jumps over the lazy dog\"   \n",
    "分解成，键值对，键为词，值为出现个数：{'the': 2, 'jumps': 1, 'lazy': 1, 'over': 1, 'fox': 1, 'dog': 1}   \n",
    "缺点是损失order of words\n",
    "##### Word2vec\n",
    "continuous bag of words (CBOW) and Skip grams  \n",
    "Skip grams：利用神经网络来训练词的向量表示，方法是：输入神经网络的一个单词预测周围的n个词。   \n",
    "训练结果具备线性相关的属性\n",
    "<img src=\"word2vec_matrix.jpg\" width=450 height=450 />\n",
    "<img src=\"word2vec_linear.jpg\" width=450 height=450 />\n",
    "##### RNN（Recurrent Neural Network）\n",
    "适合处理序列，如text和audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Intro to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### TensorFlow基础\n",
    "##### Tensor\n",
    "TensorFlow里的数据都用Tensor对象表示\n",
    "\n",
    "* 常量用tf.constant()\n",
    "* 占位符用tf.placeholder()，占位符用于运行前填入数据\n",
    "* 变量用tf.Variable()\n",
    "##### Session\n",
    "运行时的上下文环境，运行后输出Tensor的结果\n",
    "##### TensorFlow Math\n",
    "* tf.add()\n",
    "* tf.subtract()\n",
    "* tf.multiply()\n",
    "* tf.divide()\n",
    "* tf.cast()   例：tf.cast(tf.constant(2.0), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 基于TensorFlow构造神经网络做手写数字的分类\n",
    "#### 第一种神经网络：普通神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 数据集\n",
    "[MNIST数据集](http://yann.lecun.com/exdb/mnist/)：手写识别数字及其标签的数据集\n",
    "<img src=\"MNIST_Matrix.jpg\" width=450 height=450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 神经网络构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "输入层：  \n",
    "输入是28 \\* 28的矩阵  \n",
    "标签是1 \\* 9的One hot encoding向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_features = 28*28\n",
    "n_labels = 9\n",
    "x = tf.placeholder(tf.float32,(1,n_features)) # 横向量 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "隐含层：\n",
    "线性方程：$ y = xW + b $  \n",
    "这里：x输入，W是权值，b是偏差  \n",
    "方程用TensorFlow表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal((n_features, n_labels)))  \n",
    "b = tf.Variable(tf.zeros(n_labels))  \n",
    "y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "输出层：Softmax做激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "softmax = tf.nn.softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "误差函数：交叉熵  $ D(\\hat y , y) = - \\sum_j y_j ln\\hat y_j $\n",
    "<img src=\"cross_entropy_calculation.jpg\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mul_8:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_one_hot = tf.placeholder(tf.float32,shape=(9,1))\n",
    "cross_entropy = tf.multiply(-1.0, tf.reduce_sum(tf.multiply(labels_one_hot, tf.log(softmax))))\n",
    "cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 训练神经网络的技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 归一化输入和初始化权值（Normalized Inputs and Initial Weights）\n",
    "<img src=\"Mean_Variance_Image.png\" width=450 height=450 />\n",
    "输入值要进行归一化   \n",
    "权值要用正态分布随机取值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 衡量训练效果  \n",
    "训练集  \n",
    "验证集  \n",
    "测试集  \n",
    "误区：发现测试集效果不好就回头调参数，这样相当于用测试集来训练模型了，永远只在最后用测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 训练方法：随机梯度下降（Stochastic Gradient Descent）\n",
    "<img src=\"stochastic_gradient_descent.jpg\" width=450 height=450 />\n",
    "\n",
    "直接使用全部数据做梯度下降虽然下降的方向很准，但是量太大很难计算走一步太慢，还可能爆内存。   \n",
    "所以这里随机抽样一个数据出来算梯度下降近似替代。  \n",
    "这种方法可适用于大模型大数据量，应用范围广泛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Mini-batch SGD\n",
    "每次抽样一部分数据来计算梯度下降做近似\n",
    "\n",
    "[各种SGD比较](http://www.cnblogs.com/richqian/p/4549590.html)   \n",
    "batch、mini-batch、SGD、online的区别在于训练数据的选择上  \n",
    "\n",
    "| |batch|\tmini-batch|\tStochastic|\tOnline|\n",
    "|--|--|--|--|--|\n",
    "|训练集|\t固定|\t固定|\t固定|\t实时更新|\n",
    "|单次迭代样本数|\t整个训练集|\t训练集的子集|\t单个样本|\t根据具体算法定|\n",
    "|算法复杂度|\t高|\t一般|\t低|\t低|\n",
    "|时效性|\t低|\t一般（delta 模型）|\t一般（delta 模型）|\t高|\n",
    "|收敛性|\t稳定|\t较稳定|\t不稳定\t|不稳定|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 12.5     Valid Accuracy: 0.108\n",
      "Epoch: 1    - Cost: 11.5     Valid Accuracy: 0.123\n",
      "Epoch: 2    - Cost: 10.9     Valid Accuracy: 0.139\n",
      "Epoch: 3    - Cost: 10.4     Valid Accuracy: 0.153\n",
      "Epoch: 4    - Cost: 9.88     Valid Accuracy: 0.168\n",
      "Epoch: 5    - Cost: 9.45     Valid Accuracy: 0.183\n",
      "Epoch: 6    - Cost: 9.05     Valid Accuracy: 0.197\n",
      "Epoch: 7    - Cost: 8.67     Valid Accuracy: 0.212\n",
      "Epoch: 8    - Cost: 8.32     Valid Accuracy: 0.225\n",
      "Epoch: 9    - Cost: 7.99     Valid Accuracy: 0.239\n",
      "Test Accuracy: 0.24040000140666962\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Input Layer: Features and Labels\n",
    "\n",
    "features = tf.placeholder(tf.float32, shape=[None, n_input]) # (minibatch, n_input)\n",
    "labels = tf.placeholder(tf.float32, shape=[None, n_classes])  \n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal(shape=[n_input, n_classes]))  # (n_input, n_classes)\n",
    "bias = tf.Variable(tf.random_normal(shape=[n_classes]))\n",
    "\n",
    "# Hidden layer: Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias) \n",
    "# (minibatch, n_input)*(n_input, n_classes)=(minibatch, n_classes)\n",
    "# (minibatch, n_classes) + (minibatch,n_classes) = (minibatch, n_classes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 训练代数（Epochs）\n",
    "每代表示全部数据走了一边"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 训练技巧\n",
    "训练时：\n",
    "* 学习率衰减\n",
    "* 防止卡在局部最优，可用动量，记录每次梯度的值做平均后在选梯度下降的方向。\n",
    "* 学习率的选择：小的也可能更快\n",
    "<img src=\"learning_rate_tuning.jpg\" width=450 height=450>\n",
    "* 超参数的选择有点黑魔法，凭借经验，最好用的一个方法是：如果觉得效果不好，就降低学习率试试。\n",
    "* 为了降低调参的数量，可用ADAGRAD（一种SGD的变种），自带动量和学习率衰减的方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "课后练习：[Lab:TensorFlow Neural Network](https://github.com/udacity/deep-learning/tree/master/intro-to-tensorflow) 查看ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 第二种神经网络：卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "当你知道处理的是图像时，卷积神经网络最擅长处理图像"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 统计的不变性（Statistical Invariance）\n",
    "主要想说明，图像中的元素在图像的哪个位置不重要，只要能找出他识别出来即可。\n",
    "<img src=\"image_cat.jpg\" width=300 height=300 />\n",
    "\n",
    "要在神经网络里实现这种效果，要用到权值共享（Weight Sharing），猫不管在哪个位置他的权值都是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 卷积神经网络（Convolutional Neural Network）简称CovNet\n",
    "##### 简介\n",
    "图像的表示：\n",
    "<img src=\"image_representation.jpg\" width=300 height=300 />\n",
    "\n",
    "用一个小的Filter去扫描图像，filter中的weights不变，会得到一个新图像，这种操作就是卷积（Convolutions）\n",
    "<img src=\"covnet_filter.jpg\" width=300 height=300 />\n",
    "<img src=\"covnet_convolutions.jpg\" width=450 height=450 />\n",
    "卷积神经网络就是不断的做卷积叠加起来的神经网络，缩小长宽，加深深度，越深表示的信息越多\n",
    "<img src=\"covnet.jpg\" width=500 height=500 />\n",
    "最终结果放入普通的神经网络做softmax分类\n",
    "\n",
    "一些术语：  \n",
    "patch：filter的扫描的面积\n",
    "<img src=\"covnet_patch.jpg\" width=450 height=450 />\n",
    "featured map: 每个深度所表示的图像\n",
    "<img src=\"covnet_featured_map.jpg\" width=450 height=450 />\n",
    "stride: filter扫描时的步长，决定卷积后的图像的大小\n",
    "<img src=\"covnet_stride.jpg\" width=450 height=450 />\n",
    "<img src=\"covnet_stride_2.jpg\" width=450 height=450 />\n",
    "valid padding:filter扫描时不填充边缘\n",
    "<img src=\"covnet_valid_padding.jpg\" width=450 height=450 />\n",
    "same padding:filter扫描时填充边缘，使卷积后的图像同长宽\n",
    "<img src=\"covnet_same_padding.jpg\" width=450 height=450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 直观来讲：  \n",
    "每层卷积会识别图像的一种特征，更高的卷积会识别出组合后更高级的特征。  \n",
    "人也是，从最基础的特征识别，比如狗的鼻子、嘴巴，再到狗脸，最后是整个狗。\n",
    "<img src=\"covnet_intuition.jpg\" width=450 height=450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNN的核心Filter\n",
    "<img src=\"covnet_convolution_detail.jpg\" width=450 height=450 />\n",
    "Filter用Patch这么大的面积去扫整个图像，Patch里每个像素的权值是一样的（Weights Sharing），每次扫到的Patch里的像素归到一个神经元管理，\n",
    "<img src=\"covnet_filter_scan.jpg\" width=450 height=450 />\n",
    "<img src=\"covnet_filter_scan_2.jpg\" width=450 height=450 />\n",
    "<img src=\"covnet_filter_group.jpg\" width=450 height=450 />\n",
    "每一次扫描相当于把框框内的东西组合成一个神经元\n",
    "<img src=\"covnet_filter_detail.jpg\" width=300 height=300 />\n",
    "Filter的k表示抽取的特性的个数\n",
    "\n",
    "注意：如果不用卷积Filter的方式取归组像素，那相当于每个像素都要连接一个神经元，网络非常大根本无法学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习题  \n",
    "卷积神经网络中：  \n",
    "Setup  \n",
    "H = height, W = width, D = depth  \n",
    "* We have an input of shape 32x32x3 (HxWxD)  \n",
    "* 20 filters of shape 8x8x3 (HxWxD)  \n",
    "* A stride of 2 for both the height and width (S)  \n",
    "* Zero padding of size 1 (P)  \n",
    "\n",
    "Output Layer  \n",
    "* 14x14x20 (HxWxD)\n",
    "\n",
    "Q1 How many parameters does the convolutional layer have (without parameter sharing)?  \n",
    "Q2 How many parameters does the convolution layer have (with parameter sharing)?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 卷积神经网络的结构\n",
    "<img src=\"covnet_full.jpg\" width=450 height=450 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 可视化卷积神经网络\n",
    "[Visualizing and Understanding Convolutional Networks](http://www.matthewzeiler.com/pubs/arxive2013/eccv2014.pdf)\n",
    "\n",
    "<img src=\"covnet_visualize_layer1.jpg\" width=450 height=450 />\n",
    "<img src=\"covnet_visualize_layer2.jpg\" width=450 height=450 />\n",
    "<img src=\"covnet_visualize_layer3.jpg\" width=450 height=450 />\n",
    "<img src=\"covnet_visualize_layer5.jpg\" width=450 height=450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TensorFlow实现CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Output depth\n",
    "k_output = 64\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Input/Image\n",
    "input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "# Weight and bias\n",
    "weight = tf.Variable(tf.truncated_normal(\n",
    "    [filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "# Apply Convolution\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "# Add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "# Apply activation function\n",
    "conv_layer = tf.nn.relu(conv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 高级卷积神经网络网络\n",
    "**Pooling**\n",
    "<img src=\"covnet_pooling_stride_2.jpg\" width=400 height=400 />\n",
    "直接用stride步长为2的丢失很多信息，相应的我们使用stride步长为1的再用pooling抽取能保留更多信息\n",
    "<img src=\"covnet_pooling_max.jpg\" width=400 height=400 />\n",
    "最常用的pooling方式：\n",
    "* Max Pooling：取方格中最大的那个\n",
    "* Average Pooling：取每个方格的平均值\n",
    "\n",
    "Max Pooling数学表达方式：\n",
    "<img src=\"covnet_pooling_max_math.jpg\" width=400 height=400 />\n",
    "\n",
    "用途：\n",
    "* 更好的降低输出的大小\n",
    "* 防止过拟合.  Preventing overfitting is a consequence of reducing the output size, which in turn, reduces the number of parameters in future layers\n",
    "\n",
    "好处：  \n",
    "* 模型更准确  \n",
    "缺点：  \n",
    "* 中间有个stride步长为1的过程，模型会增大\n",
    "* 新增了pooling的size和stride的参数要调參\n",
    "\n",
    "**一个带Pooling层的典型卷积神经网络**\n",
    "<img src=\"covnet_pooling_classic_network.jpg\" width=450 height=450 />\n",
    "\n",
    "最近的研究表面Pooling层用的越来越少\n",
    "* 最近的数据集越来越大，我们更容易欠拟合。Recent datasets are so big and complex we're more concerned about underfitting.\n",
    "* Dropout is a much b*etter regularizer.\n",
    "* Pooling results in a loss of information. Think about the max pooling operation as an example. We only keep the largest of n numbers, thereby disregarding n-1 numbers completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')\n",
    "# The ksize and strides parameters are structured as 4-element lists, \n",
    "# with each element corresponding to a dimension of the input tensor ([batch, height, width, channels]). \n",
    "# For both ksize and strides, the batch and channel dimensions are typically set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于TensorFlow构造神经网络做手写数字的分类\n",
    "#### 第二种神经网络：卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n",
      "Epoch  1, Batch   1 -Loss: 42945.2773 Validation Accuracy: 0.136719\n",
      "Epoch  1, Batch   2 -Loss: 38002.0547 Validation Accuracy: 0.125000\n",
      "Epoch  1, Batch   3 -Loss: 29029.1250 Validation Accuracy: 0.140625\n",
      "Epoch  1, Batch   4 -Loss: 29378.3789 Validation Accuracy: 0.148438\n",
      "Epoch  1, Batch   5 -Loss: 29748.5742 Validation Accuracy: 0.156250\n",
      "Epoch  1, Batch   6 -Loss: 27914.1035 Validation Accuracy: 0.152344\n",
      "Epoch  1, Batch   7 -Loss: 22891.3438 Validation Accuracy: 0.175781\n",
      "Epoch  1, Batch   8 -Loss: 20605.0508 Validation Accuracy: 0.187500\n",
      "Epoch  1, Batch   9 -Loss: 21675.6543 Validation Accuracy: 0.183594\n",
      "Epoch  1, Batch  10 -Loss: 20485.3945 Validation Accuracy: 0.214844\n",
      "Epoch  1, Batch  11 -Loss: 21243.1582 Validation Accuracy: 0.222656\n",
      "Epoch  1, Batch  12 -Loss: 18120.5469 Validation Accuracy: 0.226562\n",
      "Epoch  1, Batch  13 -Loss: 14992.1807 Validation Accuracy: 0.253906\n",
      "Epoch  1, Batch  14 -Loss: 15436.3955 Validation Accuracy: 0.242188\n",
      "Epoch  1, Batch  15 -Loss: 14623.7236 Validation Accuracy: 0.246094\n",
      "Epoch  1, Batch  16 -Loss: 17729.4160 Validation Accuracy: 0.265625\n",
      "Epoch  1, Batch  17 -Loss: 16939.7539 Validation Accuracy: 0.269531\n",
      "Epoch  1, Batch  18 -Loss: 15174.3301 Validation Accuracy: 0.269531\n",
      "Epoch  1, Batch  19 -Loss: 16401.8594 Validation Accuracy: 0.269531\n",
      "Epoch  1, Batch  20 -Loss: 14307.3066 Validation Accuracy: 0.265625\n",
      "Epoch  1, Batch  21 -Loss: 15696.4180 Validation Accuracy: 0.269531\n",
      "Epoch  1, Batch  22 -Loss: 13644.1777 Validation Accuracy: 0.289062\n",
      "Epoch  1, Batch  23 -Loss: 13634.1680 Validation Accuracy: 0.296875\n",
      "Epoch  1, Batch  24 -Loss: 12849.9473 Validation Accuracy: 0.308594\n",
      "Epoch  1, Batch  25 -Loss: 11963.7803 Validation Accuracy: 0.324219\n",
      "Epoch  1, Batch  26 -Loss: 10764.9883 Validation Accuracy: 0.339844\n",
      "Epoch  1, Batch  27 -Loss: 11124.8398 Validation Accuracy: 0.355469\n",
      "Epoch  1, Batch  28 -Loss: 13222.0127 Validation Accuracy: 0.355469\n",
      "Epoch  1, Batch  29 -Loss:  9855.3340 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  30 -Loss:  9921.4805 Validation Accuracy: 0.378906\n",
      "Epoch  1, Batch  31 -Loss: 12777.3916 Validation Accuracy: 0.394531\n",
      "Epoch  1, Batch  32 -Loss: 11793.0078 Validation Accuracy: 0.390625\n",
      "Epoch  1, Batch  33 -Loss: 10421.4863 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  34 -Loss: 10651.2441 Validation Accuracy: 0.394531\n",
      "Epoch  1, Batch  35 -Loss:  9047.7354 Validation Accuracy: 0.414062\n",
      "Epoch  1, Batch  36 -Loss:  9276.3848 Validation Accuracy: 0.414062\n",
      "Epoch  1, Batch  37 -Loss: 11230.6953 Validation Accuracy: 0.417969\n",
      "Epoch  1, Batch  38 -Loss:  8985.7344 Validation Accuracy: 0.421875\n",
      "Epoch  1, Batch  39 -Loss: 10540.7500 Validation Accuracy: 0.433594\n",
      "Epoch  1, Batch  40 -Loss:  9214.8682 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  41 -Loss: 10966.9229 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  42 -Loss:  9342.9844 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  43 -Loss:  9794.2334 Validation Accuracy: 0.464844\n",
      "Epoch  1, Batch  44 -Loss:  8431.7080 Validation Accuracy: 0.437500\n",
      "Epoch  1, Batch  45 -Loss:  9166.8730 Validation Accuracy: 0.457031\n",
      "Epoch  1, Batch  46 -Loss:  7918.9058 Validation Accuracy: 0.480469\n",
      "Epoch  1, Batch  47 -Loss:  6114.5015 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch  48 -Loss:  7281.3755 Validation Accuracy: 0.496094\n",
      "Epoch  1, Batch  49 -Loss:  8472.7930 Validation Accuracy: 0.488281\n",
      "Epoch  1, Batch  50 -Loss:  8595.5000 Validation Accuracy: 0.496094\n",
      "Epoch  1, Batch  51 -Loss:  8214.6504 Validation Accuracy: 0.500000\n",
      "Epoch  1, Batch  52 -Loss:  7617.6763 Validation Accuracy: 0.484375\n",
      "Epoch  1, Batch  53 -Loss:  7971.5093 Validation Accuracy: 0.484375\n",
      "Epoch  1, Batch  54 -Loss:  6643.3828 Validation Accuracy: 0.488281\n",
      "Epoch  1, Batch  55 -Loss:  8060.3779 Validation Accuracy: 0.492188\n",
      "Epoch  1, Batch  56 -Loss:  6968.1562 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch  57 -Loss:  8881.5566 Validation Accuracy: 0.503906\n",
      "Epoch  1, Batch  58 -Loss:  7206.2466 Validation Accuracy: 0.500000\n",
      "Epoch  1, Batch  59 -Loss:  6422.0645 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch  60 -Loss:  5803.2676 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch  61 -Loss:  7946.0020 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch  62 -Loss:  7965.8984 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch  63 -Loss:  5972.7153 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch  64 -Loss:  5252.9648 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch  65 -Loss:  5850.5278 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch  66 -Loss:  5207.0127 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch  67 -Loss:  8176.8853 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch  68 -Loss:  5851.8818 Validation Accuracy: 0.546875\n",
      "Epoch  1, Batch  69 -Loss:  6039.5454 Validation Accuracy: 0.535156\n",
      "Epoch  1, Batch  70 -Loss:  6578.4028 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch  71 -Loss:  7777.9316 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch  72 -Loss:  5912.8916 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch  73 -Loss:  5532.6250 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch  74 -Loss:  7842.2197 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch  75 -Loss:  5391.1621 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch  76 -Loss:  7126.6904 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch  77 -Loss:  3928.4194 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch  78 -Loss:  5152.4663 Validation Accuracy: 0.546875\n",
      "Epoch  1, Batch  79 -Loss:  5827.5303 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch  80 -Loss:  7296.9355 Validation Accuracy: 0.546875\n",
      "Epoch  1, Batch  81 -Loss:  5646.4033 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch  82 -Loss:  7535.8940 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch  83 -Loss:  5041.2314 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch  84 -Loss:  6669.0703 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch  85 -Loss:  4881.0264 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch  86 -Loss:  4651.2432 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch  87 -Loss:  6319.5522 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch  88 -Loss:  6855.5249 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch  89 -Loss:  4754.4067 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch  90 -Loss:  4950.4863 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch  91 -Loss:  5152.7646 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch  92 -Loss:  5448.1357 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch  93 -Loss:  5891.8242 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch  94 -Loss:  5872.8096 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch  95 -Loss:  4295.5781 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch  96 -Loss:  5177.5703 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch  97 -Loss:  5742.6719 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch  98 -Loss:  5219.0703 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch  99 -Loss:  7035.0464 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 100 -Loss:  3965.8955 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 101 -Loss:  5016.4585 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 102 -Loss:  5095.2925 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 103 -Loss:  3714.8157 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 104 -Loss:  5577.9004 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 105 -Loss:  3933.4951 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 106 -Loss:  4648.2329 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 107 -Loss:  4445.3672 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 108 -Loss:  4231.7539 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 109 -Loss:  4273.5703 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 110 -Loss:  5975.4243 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 111 -Loss:  4373.8447 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 112 -Loss:  4240.6890 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 113 -Loss:  4147.1895 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 114 -Loss:  4434.9722 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 115 -Loss:  4666.4741 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 116 -Loss:  4525.4346 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 117 -Loss:  3761.9148 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 118 -Loss:  4349.4658 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 119 -Loss:  4099.4126 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 120 -Loss:  4839.2598 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 121 -Loss:  4218.7393 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 122 -Loss:  3936.7173 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 123 -Loss:  4682.0312 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 124 -Loss:  4934.6299 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 125 -Loss:  3346.6018 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 126 -Loss:  4869.8076 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 127 -Loss:  3455.7388 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 128 -Loss:  4236.6377 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 129 -Loss:  4519.8818 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 130 -Loss:  4462.7227 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 131 -Loss:  4991.2612 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 132 -Loss:  4032.7778 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 133 -Loss:  3692.1831 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 134 -Loss:  3109.0107 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 135 -Loss:  4259.4326 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 136 -Loss:  3923.8057 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 137 -Loss:  3238.8601 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 138 -Loss:  4450.7480 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 139 -Loss:  3831.6050 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 140 -Loss:  3805.1831 Validation Accuracy: 0.648438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1a68737022c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 keep_prob: dropout})\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linxinzhe/anaconda/envs/aind/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linxinzhe/anaconda/envs/aind/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linxinzhe/anaconda/envs/aind/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/linxinzhe/anaconda/envs/aind/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linxinzhe/anaconda/envs/aind/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Dataset You've seen this section of code from previous lessons. Here we're importing the MNIST dataset and using a convenient TensorFlow function to batch, scale, and One-Hot encode the data.\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:test_valid_size],\n",
    "                y: mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[:test_valid_size],\n",
    "        y: mnist.test.labels[:test_valid_size],\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
