{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 深度学习（Deep Learning）培训"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 根本目的：为了找出解决问题的函数。    问题-> **f**-> 解\n",
    "  \n",
    "### 找寻该函数的一种方法：神经网络，它是机器学习的其中一种方法 \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 目录\n",
    "### 神经网络（Neural Networks）\n",
    "### 卷积神经网络（Convolutional Neural Networks）\n",
    "### ➡️ 循环神经网络（Recurrent Neural Networks）⬅️\n",
    "### 生成对抗神经网络（Generative Adversarial Networks）\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 循环神经网络（Recurrent Neural Networks）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 循环神经网络入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络结构\n",
    "<img src=\"assets/rnn_steep.jpg\" height=\"450\" width=\"450\"/>\n",
    "普通的神经网络不能记录上下文的关系，而用循环神经网络因为一个样本的输出会影响下一个样本的输入，所以经过训练后能理解上下文之间关系。\n",
    "\n",
    "比如STEEP这个单词，普通神经网络是碰到E这个单词作为输入时，就懵逼了，不知道输出应该是E还是P。\n",
    "但是循环神经网络，因为知道上一个输入的是T，得知此时应该输出E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络展开\n",
    "<img src=\"assets/rnn_unroll.jpg\" width=\"450\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络实例\n",
    "<img src=\"assets/rnn_instance.jpg\" width=\"450\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 普通循环神经网络的缺点\n",
    "<img src=\"assets/rnn_drawback.jpg\" width=\"450\" height=\"450\" />\n",
    "因为每层的权重和该层上一个循环的权重相乘，上下文越长，循环层次越深，相乘越多，若小于0则越来越小而消失，若大于0则权重越来越大而爆炸。\n",
    "<img src=\"assets/rnn_drawback_plot.jpg\" width=\"450\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络的改进 - LSTM\n",
    "<img src=\"assets/rnn_cell.jpg\" width=\"450\" height=\"450\" />\n",
    "把每个循环层当作一个整体，称为cell，设计cell的结构避免梯度消失或爆炸的问题\n",
    "##### LSTM\n",
    "<img src=\"assets/rnn_lstm.jpg\"  width=\"450\" height=\"450\" />\n",
    "* 新增了一个cell state\n",
    "* 四个黄色的激活函数表示四个神经元块，每个都有各自的权重\n",
    "* 四个红色操作符的表示element-wise的操作\n",
    "##### LSTM Cell State\n",
    "<img src=\"assets/rnn_lstm_cell_state.jpg\"  width=\"450\" height=\"450\" />\n",
    "Cell State流通每层的信息，被红色操作符修改\n",
    "##### LSTM Forget Gate\n",
    "<img src=\"assets/rnn_lstm_forget_gate.jpg\"  width=\"450\" height=\"450\" />\n",
    "Sigmoid趋近于0，则忘记该信息，趋近于1则保留该信息。这样网络就能决定记住还是忘记某些信息。  \n",
    "比如，Cell State可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。\n",
    "##### LSTM Update State\n",
    "<img src=\"assets/rnn_lstm_update_state.jpg\"  width=\"450\" height=\"450\" />\n",
    "根据前一循环层输出和当前循环层输入来个更新Cell State。这里的Sigmoid同样用于保留还是忘记信息。  \n",
    "比如，在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。\n",
    "##### LSTM Cell State to Hidden Output\n",
    "<img src=\"assets/rnn_lstm_cell_state_hidden_output.jpg\"  width=\"450\" height=\"450\" />\n",
    "生成输出传入下一个循环层。这里的Sigmoid同样用于保留还是忘记信息。  \n",
    "在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。\n",
    "##### LSTM fix RNN Drawback\n",
    "<img src=\"assets/rnn_lstm_structure.jpg\"  width=\"450\" height=\"450\" />\n",
    "Sigmoid开关让信息不会爆炸\n",
    "\n",
    "##### LSTM 的变体\n",
    "改变Cell中的结构。  \n",
    "要问哪个变体是最好的？其中的差异性真的重要吗？[Greff, et al. (2015)](http://arxiv.org/pdf/1503.04069.pdf) 给出了流行变体的比较，结论是他们基本上是一样的。[Jozefowicz, et al. (2015)](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf) 则在超过 1 万种 RNN 架构上进行了测试，发现一些架构在某些任务上也取得了比 LSTM 更好的结果。\n",
    "\n",
    "详细阅读：   \n",
    "[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)   \n",
    "[[译] 理解 LSTM 网络](http://www.jianshu.com/p/9dc9f41f0b29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Character-Wise RNN\n",
    "<img src=\"assets/rnn_character_wise.jpg\"  width=\"450\" height=\"450\" />\n",
    "[Anna KaRNNa.ipynb](https://github.com/udacity/deep-learning/blob/master/intro-to-rnns/Anna_KaRNNa.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
