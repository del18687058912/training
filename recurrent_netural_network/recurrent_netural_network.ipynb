{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 深度学习（Deep Learning）培训"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 根本目的：为了找出解决问题的函数。    问题-> **f**-> 解\n",
    "  \n",
    "### 找寻该函数的一种方法：神经网络，它是机器学习的其中一种方法 \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 目录\n",
    "### 神经网络（Neural Networks）\n",
    "### 卷积神经网络（Convolutional Neural Networks）\n",
    "### ➡️ 循环神经网络（Recurrent Neural Networks）⬅️\n",
    "### 生成对抗神经网络（Generative Adversarial Networks）\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 循环神经网络（Recurrent Neural Networks）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 循环神经网络入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络结构\n",
    "<img src=\"assets/rnn_steep.jpg\" height=\"450\" width=\"450\"/>\n",
    "普通的神经网络不能记录上下文的关系，而用循环神经网络因为一个样本的输出会影响下一个样本的输入，所以经过训练后能理解上下文之间关系。\n",
    "\n",
    "比如STEEP这个单词，普通神经网络是碰到E这个单词作为输入时，就懵逼了，不知道输出应该是E还是P。\n",
    "但是循环神经网络，因为知道上一个输入的是T，得知此时应该输出E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络展开\n",
    "<img src=\"assets/rnn_unroll.jpg\" width=\"450\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络实例\n",
    "<img src=\"assets/rnn_instance.jpg\" width=\"450\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 普通循环神经网络的缺点\n",
    "<img src=\"assets/rnn_drawback.jpg\" width=\"450\" height=\"450\" />\n",
    "因为每层的权重和该层上一个循环的权重相乘，上下文越长，循环层次越深，相乘越多，若小于0则越来越小而消失，若大于0则权重越来越大而爆炸。\n",
    "<img src=\"assets/rnn_drawback_plot.jpg\" width=\"450\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络的改进 - LSTM\n",
    "<img src=\"assets/rnn_cell.jpg\" width=\"450\" height=\"450\" />\n",
    "把每个循环层的每个神经元替换成Cell，设计cell的结构避免梯度消失或爆炸的问题\n",
    "##### LSTM\n",
    "<img src=\"assets/rnn_lstm.jpg\"  width=\"450\" height=\"450\" />\n",
    "* 新增了一个cell state\n",
    "* 四个黄色的激活函数表示四个神经元块，每个都有各自的权重\n",
    "* 四个红色操作符的表示element-wise的操作\n",
    "##### LSTM Cell State\n",
    "<img src=\"assets/rnn_lstm_cell_state.jpg\"  width=\"450\" height=\"450\" />\n",
    "Cell State流通每层的信息，被红色操作符修改\n",
    "##### LSTM Forget Gate\n",
    "<img src=\"assets/rnn_lstm_forget_gate.jpg\"  width=\"450\" height=\"450\" />\n",
    "Sigmoid趋近于0，则忘记该信息，趋近于1则保留该信息。这样网络就能决定记住还是忘记某些信息。  \n",
    "比如，Cell State可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。\n",
    "##### LSTM Update State\n",
    "<img src=\"assets/rnn_lstm_update_state.jpg\"  width=\"450\" height=\"450\" />\n",
    "<img src=\"assets/rnn_lstm_update_state_analogy.jpg\"  width=\"450\" height=\"450\" />\n",
    "根据前一循环层输出和当前循环层输入来个更新Cell State。这里的Sigmoid同样用于保留还是忘记信息。  \n",
    "比如，在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。\n",
    "##### LSTM Cell State to Hidden Output\n",
    "<img src=\"assets/rnn_lstm_cell_state_hidden_output.jpg\"  width=\"450\" height=\"450\" />\n",
    "生成输出传入下一个循环层。这里的Sigmoid同样用于保留还是忘记信息。  \n",
    "在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。\n",
    "##### LSTM fix RNN Drawback\n",
    "<img src=\"assets/rnn_lstm_structure.jpg\"  width=\"450\" height=\"450\" />\n",
    "Sigmoid开关让信息不会爆炸\n",
    "\n",
    "##### LSTM 的变体\n",
    "改变Cell中的结构。  \n",
    "要问哪个变体是最好的？其中的差异性真的重要吗？[Greff, et al. (2015)](http://arxiv.org/pdf/1503.04069.pdf) 给出了流行变体的比较，结论是他们基本上是一样的。[Jozefowicz, et al. (2015)](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf) 则在超过 1 万种 RNN 架构上进行了测试，发现一些架构在某些任务上也取得了比 LSTM 更好的结果。\n",
    "\n",
    "详细阅读：   \n",
    "[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)   \n",
    "[[译] 理解 LSTM 网络](http://www.jianshu.com/p/9dc9f41f0b29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Character-Wise RNN\n",
    "<img src=\"assets/rnn_character_wise.jpg\"  width=\"450\" height=\"450\" />\n",
    "[Anna KaRNNa.ipynb](https://github.com/udacity/deep-learning/blob/master/intro-to-rnns/Anna_KaRNNa.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "不存在通用的超参数\n",
    "超参数有两类：\n",
    "1. 优化器超参数： 训练相关的，如：learning rate、mini-batch size、epochs\n",
    "2. 模型的超参数：RNN的lstm_size，CNN的kernel_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Learning Rate\n",
    "好的起点：0.01  \n",
    "常用：\n",
    "* 0.1\n",
    "* 0.01\n",
    "* 0.001\n",
    "* 0.0001\n",
    "* 0.00001\n",
    "* 0.000001  \n",
    "\n",
    "<img src=\"assets/hyperparameters_learning_rate.jpg\" width=\"650\" height=\"650\" />\n",
    "常见Learning Rate效果\n",
    "\n",
    "<img src=\"assets/hyperparameters_learning_rate_vibrate.jpg\" width=\"350\" height=\"350\" />\n",
    "如果学习率导致训练卡住了，可以用[学习率衰减](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay)解决，简单的方法即随时间衰减，复杂点的按数据的变化衰减，如AdamOptimizer、AdagradOptimizer\n",
    "\n",
    "参考：  \n",
    "[Exponential Decay](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay) in TensorFlow.  \n",
    "Adaptive Learning Optimizers  \n",
    "* [AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)\n",
    "* [AdagradOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Mini-batch\n",
    "batch、mini-batch、SGD、online的区别在于训练数据的选择上  \n",
    "\n",
    "| |batch|\tmini-batch|\tStochastic|\tOnline|\n",
    "|--|--|--|--|--|\n",
    "|训练集|\t固定|\t固定|\t固定|\t实时更新|\n",
    "|单次迭代样本数|\t整个训练集|\t训练集的子集|\t单个样本|\t根据具体算法定|\n",
    "|算法复杂度|\t高|\t一般|\t低|\t低|\n",
    "|时效性|\t低|\t一般（delta 模型）|\t一般（delta 模型）|\t高|\n",
    "|收敛性|\t稳定|\t较稳定|\t不稳定\t|不稳定|\n",
    "\n",
    "好的起点：32  \n",
    "好的尝试：32、64、128、256  \n",
    "常用：\n",
    "1、2、4、8、16、32、64、128、256   \n",
    "\n",
    "大的数字可以利用矩阵运算加速，但内存也会要求更高  \n",
    "小的数字有噪声的随机性对于走出局部最优有好处  \n",
    "\n",
    "注意：  \n",
    "记得，修改mini-batch的数字也要相应修改学习率，不然精度可能下降\n",
    "\n",
    "参考：  \n",
    "[Systematic evaluation of CNN advances on the ImageNet](https://arxiv.org/abs/1606.02228) by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Epochs\n",
    "\n",
    "<img src=\"assets/hyperparameters_epochs_early_stop.jpg\" width=\"350\" height=\"350\" />\n",
    "关键点在于Validation Error，只要Validation Error还在降低，就继续迭代，直到看到误差在上升，而且观察一段时间不改善就可以停止了。\n",
    "\n",
    "参考：  \n",
    "[SessionRunHook](https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook):SessionRunHooks are an evolving part of tf.train, and going forward appear to be the proper place where you'd implement early stopping.  \n",
    "[Training Hooks](https://www.tensorflow.org/api_guides/python/train#Training_Hooks):  \n",
    "* [StopAtStepHook](https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook): A monitor to request the training stop after a certain number of steps  \n",
    "* [NanTensorHook](https://www.tensorflow.org/api_docs/python/tf/train/NanTensorHook): a monitor that monitor's loss and stops training if it encounters a NaN loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Number of Hidden Units/ Layers\n",
    "\n",
    "<img src=\"assets/hyperparameters_number_of_hidden_units_overfitting.jpg\" width=\"550\" height=\"550\" />\n",
    "选择足够的Hidden Units来训练数据，但要小心模型太大导致过拟合。\n",
    "如果Validation Error和Training Error差距够大，则表示过拟合了\n",
    "\n",
    "\n",
    "<img src=\"assets/hyperparameters_number_of_hidden_units_prevent_overfitting.jpg\" width=\"550\" height=\"550\" />\n",
    "也可以使用Dropout和L2 Regularization防止过拟合\n",
    "\n",
    "好的开始：\n",
    "1. 设置为比输入的数目稍大一点的数\n",
    "2. 三层比两层好，CNN除外"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Embeddings and Word2vec\n",
    "[Skip-Grams-Solution.ipynb](https://github.com/udacity/deep-learning/blob/master/embeddings/Skip-Grams-Solution.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Siraj's Style Transfer\n",
    "VGG16 Net是一个pre-trained的网络，可以抽象的你输入给他的图像。可以基于他建立自己的图像处理的神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TensorBoard\n",
    "\n",
    "#### 查看模型\n",
    "```\n",
    "with tf.Session() as sess:    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/1', sess.graph)\n",
    "```\n",
    "\n",
    "<img src=\"./assets/tensorboard_graph.jpg\" width=\"450\" height=\"450\"/>\n",
    "\n",
    "#### 模型归组\n",
    "```\n",
    "with tf.name_scope(\"RNN_layers\"):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "```\n",
    "<img src=\"./assets/tensorboard_graph_grouped.jpg\" width=\"350\" height=\"350\"/>\n",
    "\n",
    "#### 模型参数采样\n",
    "```\n",
    " with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "....\n",
    "merged = tf.summary.merge_all()  #记得把采集点归拢成一个\n",
    ".....\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/2/train', sess.graph)\n",
    "....\n",
    "summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                          model.final_state, model.optimizer], \n",
    "                                                          feed_dict=feed)\n",
    "....\n",
    "train_writer.add_summary(summary, iteration) # 每次循环在归拢点集中采样\n",
    "```\n",
    "tf.summary.histogram设置需要记录的点，运行时每次train_writer.add_summary做记录\n",
    "<img src=\"./assets/tensorboard_collect_variables_histogram.jpg\" width=\"450\" height=\"450\"/>\n",
    "观察histogram的分布是否变化就知道权值是否有更新也就是是否训练的起来\n",
    "\n",
    "比如tf.summary.scalar('cost', cost)采集误差\n",
    "<img src=\"./assets/tensorboard_collect_variables_scalar.jpg\" width=\"450\" height=\"450\"/>\n",
    "\n",
    "```\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "for lstm_size in [128,256,512]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for learning_rate in [0.002, 0.001]:\n",
    "            log_string = 'logs/4/lr={},rl={},ru={}'.format(learning_rate, num_layers, lstm_size)\n",
    "            writer = tf.summary.FileWriter(log_string)\n",
    "            model = build_rnn(len(vocab), \n",
    "                    batch_size=batch_size,\n",
    "                    num_steps=num_steps,\n",
    "                    learning_rate=learning_rate,\n",
    "                    lstm_size=lstm_size,\n",
    "                    num_layers=num_layers)\n",
    "            \n",
    "            train(model, epochs, writer)\n",
    "```\n",
    "对每个参数配置做记录，最终可以得到他们之间对比的图案\n",
    "<img src=\"./assets/tensorboard_select_hyperparameters.jpg\" width=\"450\" height=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Siraj's Music Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Weight Initialization\n",
    "[weight_initialization.ipynb](https://github.com/udacity/deep-learning/blob/master/weight-initialization/weight_initialization.ipynb)\n",
    "\n",
    "#### 全1或全0初始化\n",
    "After 858 Batches (2 Epochs):  \n",
    "Validation Accuracy  \n",
    "   11.260% -- All Zeros  \n",
    "    9.900% -- All Ones  \n",
    "Loss  \n",
    "    2.300  -- All Zeros  \n",
    "  372.644  -- All Ones  \n",
    "全1和全0的方式都不好，因为大家都一样，反向传播算法不知道更新哪一个  \n",
    "<img src=\"./assets/weights_init_zeros_ones.jpg\" width=\"450\" height=\"450\"/>\n",
    "\n",
    "#### Uniform Distribution\n",
    "After 858 Batches (2 Epochs):  \n",
    "Validation Accuracy  \n",
    "   65.340% -- tf.random_uniform [0, 1)  \n",
    "Loss  \n",
    "   64.356  -- tf.random_uniform [0, 1)  \n",
    "<img src=\"./assets/weights_init_uniform_distribution.jpg\" width=\"450\" height=\"450\"/>\n",
    "\n",
    "#### General rule for setting weights\n",
    "通用的方法是，设置一个0左右的不太小的区间。\n",
    "A good pracitce is to start your weights in the range of [−y,y] where $ y = \\frac1{\\sqrt{n}} $\n",
    "这里的n is the number of inputs to a given neuron).\n",
    "<img src=\"./assets/weights_init_symmetry_0.jpg\" width=\"450\" height=\"450\"/>\n",
    "\n",
    "After 858 Batches (2 Epochs):  \n",
    "Validation Accuracy  \n",
    "   91.000% -- [-1, 1)  \n",
    "   97.220% -- [-0.1, 0.1)  \n",
    "   95.680% -- [-0.01, 0.01)  \n",
    "   94.400% -- [-0.001, 0.001)  \n",
    "Loss  \n",
    "    2.425  -- [-1, 1)  \n",
    "    0.098  -- [-0.1, 0.1)  \n",
    "    0.133  -- [-0.01, 0.01)  \n",
    "    0.190  -- [-0.001, 0.001)  \n",
    "如果设置的太小会有问题，准确率也下降了  \n",
    "\n",
    "#### Normal Distribution\n",
    "After 858 Batches (2 Epochs):  \n",
    "Validation Accuracy  \n",
    "   96.920% -- Uniform [-0.1, 0.1)  \n",
    "   97.200% -- Normal stddev 0.1  \n",
    "Loss  \n",
    "    0.103  -- Uniform [-0.1, 0.1)  \n",
    "    0.099  -- Normal stddev 0.1  \n",
    "稍微有所提高  \n",
    "<img src=\"./assets/weights_init_normal_distribution.jpg\" width=\"450\" height=\"450\"/>\n",
    "\n",
    "#### Truncated Normal Distribution\n",
    "After 858 Batches (2 Epochs):  \n",
    "Validation Accuracy  \n",
    "   97.020% -- Normal  \n",
    "   97.480% -- Truncated Normal  \n",
    "Loss  \n",
    "    0.088  -- Normal  \n",
    "    0.034  -- Truncated Normal  \n",
    "模型再大点差别会更明显，因为正态分布有些过大过小的数会影响模型，而截断他就少受影响\n",
    "<img src=\"./assets/weights_init_truncated_normal_distribution.jpg\" width=\"450\" height=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setiment Prediction RNN\n",
    "[Sentiment_RNN_Solution.ipynb](https://github.com/udacity/deep-learning/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Transfer Learning\n",
    "[Transfer_Learning_Solution.ipynb](https://github.com/udacity/deep-learning/blob/master/transfer-learning/Transfer_Learning_Solution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Siraj's Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
