{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 深度学习（Deep Learning）培训"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 根本目的：为了找出解决问题的函数。    问题-> **f**-> 解\n",
    "  \n",
    "### 找寻该函数的一种方法：神经网络，它是机器学习的其中一种方法 \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 目录\n",
    "### 神经网络（Neural Networks）\n",
    "### 卷积神经网络（Convolutional Neural Networks）\n",
    "### ➡️ 循环神经网络（Recurrent Neural Networks）⬅️\n",
    "### 生成对抗神经网络（Generative Adversarial Networks）\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 循环神经网络（Recurrent Neural Networks）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 循环神经网络入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络结构\n",
    "<img src=\"assets/rnn_steep.jpg\" height=\"450\" width=\"450\"/>\n",
    "普通的神经网络不能记录上下文的关系，而用循环神经网络因为一个样本的输出会影响下一个样本的输入，所以经过训练后能理解上下文之间关系。\n",
    "\n",
    "比如STEEP这个单词，普通神经网络是碰到E这个单词作为输入时，就懵逼了，不知道输出应该是E还是P。\n",
    "但是循环神经网络，因为知道上一个输入的是T，得知此时应该输出E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络展开\n",
    "<img src=\"assets/rnn_unroll.jpg\" width=\"450\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络实例\n",
    "<img src=\"assets/rnn_instance.jpg\" width=\"450\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 普通循环神经网络的缺点\n",
    "<img src=\"assets/rnn_drawback.jpg\" width=\"450\" height=\"450\" />\n",
    "因为每层的权重和该层上一个循环的权重相乘，上下文越长，循环层次越深，相乘越多，若小于0则越来越小而消失，若大于0则权重越来越大而爆炸。\n",
    "<img src=\"assets/rnn_drawback_plot.jpg\" width=\"450\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 循环神经网络的改进 - LSTM\n",
    "<img src=\"assets/rnn_cell.jpg\" width=\"450\" height=\"450\" />\n",
    "把每个循环层的每个神经元替换成Cell，设计cell的结构避免梯度消失或爆炸的问题\n",
    "##### LSTM\n",
    "<img src=\"assets/rnn_lstm.jpg\"  width=\"450\" height=\"450\" />\n",
    "* 新增了一个cell state\n",
    "* 四个黄色的激活函数表示四个神经元块，每个都有各自的权重\n",
    "* 四个红色操作符的表示element-wise的操作\n",
    "##### LSTM Cell State\n",
    "<img src=\"assets/rnn_lstm_cell_state.jpg\"  width=\"450\" height=\"450\" />\n",
    "Cell State流通每层的信息，被红色操作符修改\n",
    "##### LSTM Forget Gate\n",
    "<img src=\"assets/rnn_lstm_forget_gate.jpg\"  width=\"450\" height=\"450\" />\n",
    "Sigmoid趋近于0，则忘记该信息，趋近于1则保留该信息。这样网络就能决定记住还是忘记某些信息。  \n",
    "比如，Cell State可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。\n",
    "##### LSTM Update State\n",
    "<img src=\"assets/rnn_lstm_update_state.jpg\"  width=\"450\" height=\"450\" />\n",
    "根据前一循环层输出和当前循环层输入来个更新Cell State。这里的Sigmoid同样用于保留还是忘记信息。  \n",
    "比如，在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。\n",
    "##### LSTM Cell State to Hidden Output\n",
    "<img src=\"assets/rnn_lstm_cell_state_hidden_output.jpg\"  width=\"450\" height=\"450\" />\n",
    "生成输出传入下一个循环层。这里的Sigmoid同样用于保留还是忘记信息。  \n",
    "在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。\n",
    "##### LSTM fix RNN Drawback\n",
    "<img src=\"assets/rnn_lstm_structure.jpg\"  width=\"450\" height=\"450\" />\n",
    "Sigmoid开关让信息不会爆炸\n",
    "\n",
    "##### LSTM 的变体\n",
    "改变Cell中的结构。  \n",
    "要问哪个变体是最好的？其中的差异性真的重要吗？[Greff, et al. (2015)](http://arxiv.org/pdf/1503.04069.pdf) 给出了流行变体的比较，结论是他们基本上是一样的。[Jozefowicz, et al. (2015)](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf) 则在超过 1 万种 RNN 架构上进行了测试，发现一些架构在某些任务上也取得了比 LSTM 更好的结果。\n",
    "\n",
    "详细阅读：   \n",
    "[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)   \n",
    "[[译] 理解 LSTM 网络](http://www.jianshu.com/p/9dc9f41f0b29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Character-Wise RNN\n",
    "<img src=\"assets/rnn_character_wise.jpg\"  width=\"450\" height=\"450\" />\n",
    "[Anna KaRNNa.ipynb](https://github.com/udacity/deep-learning/blob/master/intro-to-rnns/Anna_KaRNNa.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "不存在通用的超参数\n",
    "超参数有两类：\n",
    "1. 优化器超参数： 训练相关的，如：learning rate、mini-batch size、epochs\n",
    "2. 模型的超参数：RNN的lstm_size，CNN的kernel_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Learning Rate\n",
    "好的起点：0.01  \n",
    "常用：\n",
    "* 0.1\n",
    "* 0.01\n",
    "* 0.001\n",
    "* 0.0001\n",
    "* 0.00001\n",
    "* 0.000001  \n",
    "\n",
    "<img src=\"assets/hyperparameters_learning_rate.jpg\" width=\"650\" height=\"650\" />\n",
    "常见Learning Rate效果\n",
    "\n",
    "<img src=\"assets/hyperparameters_learning_rate_vibrate.jpg\" width=\"350\" height=\"350\" />\n",
    "如果学习率导致训练卡住了，可以用[学习率衰减](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay)解决，简单的方法即随时间衰减，复杂点的按数据的变化衰减，如AdamOptimizer、AdagradOptimizer\n",
    "\n",
    "参考：  \n",
    "[Exponential Decay](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay) in TensorFlow.  \n",
    "Adaptive Learning Optimizers  \n",
    "* [AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)\n",
    "* [AdagradOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Mini-batch\n",
    "batch、mini-batch、SGD、online的区别在于训练数据的选择上  \n",
    "\n",
    "| |batch|\tmini-batch|\tStochastic|\tOnline|\n",
    "|--|--|--|--|--|\n",
    "|训练集|\t固定|\t固定|\t固定|\t实时更新|\n",
    "|单次迭代样本数|\t整个训练集|\t训练集的子集|\t单个样本|\t根据具体算法定|\n",
    "|算法复杂度|\t高|\t一般|\t低|\t低|\n",
    "|时效性|\t低|\t一般（delta 模型）|\t一般（delta 模型）|\t高|\n",
    "|收敛性|\t稳定|\t较稳定|\t不稳定\t|不稳定|\n",
    "\n",
    "好的起点：32  \n",
    "好的尝试：32、64、128、256  \n",
    "常用：\n",
    "1、2、4、8、16、32、64、128、256   \n",
    "\n",
    "大的数字可以利用矩阵运算加速，但内存也会要求更高  \n",
    "小的数字有噪声的随机性对于走出局部最优有好处  \n",
    "\n",
    "注意：  \n",
    "记得，修改mini-batch的数字也要相应修改学习率，不然精度可能下降\n",
    "\n",
    "参考：  \n",
    "[Systematic evaluation of CNN advances on the ImageNet](https://arxiv.org/abs/1606.02228) by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs\n",
    "\n",
    "<img src=\"assets/hyperparameters_epochs_early_stop.jpg\" width=\"350\" height=\"350\" />\n",
    "关键点在于Validation Error，只要Validation Error还在降低，就继续迭代，直到看到误差在上升，而且观察一段时间不改善就可以停止了。\n",
    "\n",
    "参考：  \n",
    "[SessionRunHook](https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook):SessionRunHooks are an evolving part of tf.train, and going forward appear to be the proper place where you'd implement early stopping.  \n",
    "[Training Hooks](https://www.tensorflow.org/api_guides/python/train#Training_Hooks):  \n",
    "* [StopAtStepHook](https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook): A monitor to request the training stop after a certain number of steps  \n",
    "* [NanTensorHook](https://www.tensorflow.org/api_docs/python/tf/train/NanTensorHook): a monitor that monitor's loss and stops training if it encounters a NaN loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Hidden Units/ Layers\n",
    "\n",
    "<img src=\"assets/hyperparameters_number_of_hidden_units_overfitting.jpg\" width=\"550\" height=\"550\" />\n",
    "选择足够的Hidden Units来训练数据，但要小心模型太大导致过拟合。\n",
    "如果Validation Error和Training Error差距够大，则表示过拟合了\n",
    "\n",
    "\n",
    "<img src=\"assets/hyperparameters_number_of_hidden_units_prevent_overfitting.jpg\" width=\"550\" height=\"550\" />\n",
    "也可以使用Dropout和L2 Regularization防止过拟合\n",
    "\n",
    "好的开始：\n",
    "1. 设置为比输入的数目稍大一点的数\n",
    "2. 三层比两层好，CNN除外"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
